{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import datetime\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "#import pyspark libraries\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# import sql Spark libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StringType, IntegerType,DataType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import min as Fmin\n",
    "from pyspark.sql.functions import max as Fmax\n",
    "from pyspark.sql.functions import avg, col, min, max, regexp_replace, concat, count, desc, asc, explode, lit, split, stddev, udf, lower, isnan, when, rank, from_unixtime\n",
    "\n",
    "# import ml Spark libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# import sklearn ml metrics libraries for metrics calculation\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"My Sparkify app\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1617139075698'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '46533'),\n",
       " ('spark.driver.host', '5488a941163e'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'My Sparkify app')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the Spark context\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5488a941163e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My Sparkify app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe792913d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mini-dataset file\n",
    "file = \"./mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the dataset\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare some functions that we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a function to count the nulls/missing/empty values\n",
    "def count_null_col(df,column):\n",
    "    '''\n",
    "    Function that counts the nulls or missing values that exist in a column\n",
    "    Input:  \n",
    "            df: dataset where we want to count null/missing/empty values\n",
    "            column: column of the dataset we want to count null/missing/empty values\n",
    "    Output: \n",
    "            n_missing: it returns the number of missing values\n",
    "    '''\n",
    "    missing = df.filter(((df[column].isNull()) | isnan(df[column])) | (df[column] == \"\"))\n",
    "    n_missing = missing.count()\n",
    "    \n",
    "    return n_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a function to count the nulls/missing/empty values of every column of the dataset\n",
    "def count_null_df(df):\n",
    "    '''\n",
    "    Function that counts the nulls or missing values that exist in a dataset\n",
    "    Input:  \n",
    "            df: dataset where we want to count null/missing/empty values\n",
    "    Output: \n",
    "            missing: it returns a dictionary with the number of missing values of every column\n",
    "            in case the column has nulls/missing/empty values\n",
    "    '''   \n",
    "    missing = {}\n",
    "    for col in df.columns:\n",
    "        n_missing = count_null_col(df,col)\n",
    "        if n_missing > 0:\n",
    "            missing.update({col: n_missing})\n",
    "    \n",
    "    return missing      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a function to count if the categorical and numerical columns of the dataset\n",
    "def cat_or_num(df):\n",
    "    '''\n",
    "    Function that clasifies the kind of columns (categorical or numerical) that exist in a dataset\n",
    "    Input:  \n",
    "            df: dataset where we want to clasify columns\n",
    "    Output: \n",
    "            num_cols,cat:cols: it returns a dictionary with the category of every column\n",
    "    \n",
    "    '''   \n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "\n",
    "    for coltype in df.schema:\n",
    "        ctype = str(coltype.dataType)\n",
    "        if ctype == \"StringType\":\n",
    "            cat_cols.append(coltype.name)\n",
    "\n",
    "        elif ctype == \"LongType\" or ctype == \"DoubleType\":\n",
    "            num_cols.append(coltype.name)\n",
    "            \n",
    "    return cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about the cols and features\n",
    "def show_cols_info(df):\n",
    "    '''\n",
    "    Function that shows the summary of the information of the columns of the datasets\n",
    "    Input:  \n",
    "            df: dataset we want to show information \n",
    "    Output: \n",
    "            none: it prints a summary and the main values of every column\n",
    "    \n",
    "    '''     \n",
    "\n",
    "    for column in df.columns:\n",
    "        # show a summary of the important information of the column\n",
    "        df.describe([column]).show()\n",
    "        # show the different values of the field\n",
    "        df.select([column]).distinct().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many missing values there are in the dataset\n",
    "missing = count_null_df(df)\n",
    "print(\"These are the columns with nulls/missing/empty values: {}\\n\".format(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Rows with Missing Values\n",
    "As you'll see, it turns out there are no missing values in session column, but there are values that are empty string. Also there are userID values that are empty strings. Let's detect and delete them as these rows are not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the unique records of UserId\n",
    "df.select(\"userId\").dropDuplicates().sort(\"userId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After viewing the previous query, we can see there are some users with empty UserId\n",
    "# Let's eliminate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid.filter(df_valid[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's count the number of records after remove the empty strings from userId field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And check again if there are no empty field in userId column\n",
    "df_valid.select(\"userId\").dropDuplicates().sort(\"userId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this usecase, I am going to define Churn as the Cancellation Confirmation event appears\n",
    "And now, let's start with the EDA phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take an overview of the mini dataset file -->  18 fields \n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a overview of the schema of the dataset file\n",
    "df_valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanations of the fields:\n",
    " \n",
    " |-- artist: singer of the song\n",
    " |-- auth: method of accessing sparkify          (values: Logged In/Cancelled)\n",
    " |-- firstName: first name of the client\n",
    " |-- gender: gender of the client                (values: M/F)\n",
    " |-- itemInSession: item count in a session\n",
    " |-- lastName: last name of the client\n",
    " |-- length: lenght of the song\n",
    " |-- level: account contract type                (values: free/paid)\n",
    " |-- location: main location of the client\n",
    " |-- method: method for accessing sparkify (put, get, etc..)   (values: PUT/GET)\n",
    " |-- page: page the user visits\n",
    " |-- registration: time of the registration (unix timestamp)\n",
    " |-- sessionId: identification of the session\n",
    " |-- song: song the client has listened\n",
    " |-- status: status of the accessing method (http status) (values: 307/404/200)\n",
    " |-- ts: information about the time of the user event (unix timestamp)\n",
    " |-- userAgent: browser the user has accessed to sparkify with\n",
    " |-- userId: identification for the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's categorize the columns, depending on their type (categorical or numerical)\n",
    "cat_cols,num_cols = cat_or_num(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The categorical columns are: {}\".format(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The numerical columns are: {}\".format(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many registries the file has --> we have 278.154 records\n",
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first lines of the file, to see what they look like\n",
    "df_valid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see just one row\n",
    "df_valid.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have an statistic overview of the dataset --> but with this view it is difficult to see...\n",
    "df_valid.describe().show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's better go field by field \n",
    "df_valid.describe('artist').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('auth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two values for auth = Logged In or Cancelled\n",
    "df_valid.select('auth').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('firstName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('gender').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('itemInSession').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('lastName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('level').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('level').distinct().collect() #--> two values: free or paid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('location').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the different locations, it is a string composed by two parts separated by a comma: \n",
    "# the first one is the name of the location, the second one after the comma is the abbreviation of the location\n",
    "# for an easier management of this information, we will use the second part\n",
    "\n",
    "df_valid.select('location').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for a better visualization of the location column\n",
    "# The location column has the form: Name,Short_Name, eg: 'Lexington-Fayette, KY'\n",
    "# so we'll get the column location and split it by comma and then get the second part\n",
    "# and then we'll take only the first part\n",
    "df_valid = df_valid.withColumn(\"state\", split(col(\"location\"),',').getItem(1))\n",
    "#df_valid = df_valid.withColumn(\"state\", split(col(\"state\"),'-').getItem(1))\n",
    "df_valid.select(\"state\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('method').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('method').distinct().collect() # there are two methods: PUT/GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('page').distinct().collect() # there are 19 different pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('registration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('sessionId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('song').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('status').distinct().collect() # there are 3 possible status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('ts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('userAgent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select(\"userID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many distinct users are in this dataset --> 225\n",
    "df_valid.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's see how many different pages are --> 19\n",
    "df_valid.select(\"page\").dropDuplicates().sort(\"page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see an example of row in the dataset, for instance let's see userid= 30\n",
    "df_valid.select([\"UserId\", \"firstname\", \"page\", \"song\"]).where(df_valid.userId==\"30\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to add two new columns derived from ts field:\n",
    "# We would like to know the hour and datetime in UTF timestamp\n",
    "# for that we prepare a lambda function\n",
    "get_date       = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "get_month      = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). month)\n",
    "get_day        = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). day)\n",
    "get_hour       = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). hour)\n",
    "get_weekday    = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime('%w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply the lambda function to convert the ts into hour and data\n",
    "# we apply also the lambda function to convert the registration field into a new field \"registration_time\" with the timestamp \n",
    "df_valid = df_valid.withColumn(\"registration_time\", get_date(df_valid.registration))\n",
    "df_valid = df_valid.withColumn(\"date\", get_date(df_valid.ts))\n",
    "df_valid = df_valid.withColumn(\"month\", get_month(df_valid.ts))\n",
    "df_valid = df_valid.withColumn(\"day_of_month\", get_day(df_valid.ts))\n",
    "df_valid = df_valid.withColumn(\"hour\", get_hour(df_valid.ts))\n",
    "df_valid = df_valid.withColumn('day_of_week', get_weekday(df_valid.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare sparksql \n",
    "# To do that, we need to create a temporary table, where we'll perform the SQL queries\n",
    "df_valid.createOrReplaceTempView(\"Sparkify_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the registration_time for human understanding, that is year-month-day hour:minute:second, \n",
    "# And also let's add the weekday\n",
    "df_time = df_valid.select('registration_time', date_format('registration_time', 'u').alias('weekday'))\n",
    "df_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate how long the user is subscribed to our Sparkify service\n",
    "# To calculate that, we have to substract current date to the registration date\n",
    "\n",
    "def get_days_from_registration(df_valid):\n",
    "    days_from_registration = df_valid.ts - df_valid.registration\n",
    "    days_from_registration = days_from_registration/(1000*3600*24)\n",
    "    \n",
    "    return days_from_registration\n",
    "    \n",
    "    \n",
    "df_valid = df_valid.withColumn(\"subscription_days\", get_days_from_registration(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('subscription_days').sort(\"subscription_days\").collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('userID', 'subscription_days').take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we continue with our Data Exploration! Let's do it just making questions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question: How many days/months does our dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a dataset with an initial date = 2018-10-01\n",
    "day_from = list(df_valid.select('date').sort(\"date\").collect()[0])\n",
    "day_from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a dataset with an end date = 2018-12-03\n",
    "day_to = list(df_valid.select('date').sort(\"date\").collect()[-1])\n",
    "day_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We have a dataset with an initial date = 2018-10-01 and an end date = 2018-12-03\n",
    "#### So we have a dataset of aprox 2 months information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the states\n",
    "df_valid.select('userId','state').distinct().groupby('state').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How is the proportion between Males and Females in our musical streaming service ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngender = df_valid.select('gender','userId').distinct().groupby('gender').count().toPandas()\n",
    "ngender.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot this proportion with a graphic\n",
    "# and show the percentage above\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.title('Number of Users by Gender & Percentage')\n",
    "ax = sns.barplot(x='gender', y='count',hue='gender', data=ngender);\n",
    "\n",
    "# this code is to calculate the percentage \n",
    "# and present the numbers above the bars and centered in the middle\n",
    "total = ngender.iloc[0]['count'] + ngender.iloc[1]['count']\n",
    "for p in ax.patches:\n",
    "    if p.get_height()>0:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width()/2.6\n",
    "        y = p.get_y() + p.get_height() + 0.7\n",
    "        ax.annotate(percentage, (x, y))\n",
    "\n",
    "plt.xticks(size=12)\n",
    "plt.xlabel('Gender',size=12)\n",
    "plt.yticks(size=12)\n",
    "plt.ylabel('Number of Users', size=12)\n",
    "plt.legend(title='Gender', loc='best');\n",
    "plt.savefig('Users_by_Gender.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our datasets contains 104 records corresponding to female clients (46.2%) and 121 (53.8%) corresponding to male clients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What about the level of the users? Is there a difference between male and female subscriptors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check the users by level of account and gender\n",
    "# As we are going to use some sns plots, we have to convert the datasets to Pandas\n",
    "df_gender_level = df_valid.select('userId','gender','level').distinct().groupby('gender','level').count().toPandas().sort_values(by='count')\n",
    "df_gender_level.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For both free and paid services, the number of men is sligthly higher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "ax = sns.barplot(x='level', y='count',hue='gender', data=df_gender_level);\n",
    "\n",
    "plt.title('Number of Users by Gender & Percentage')\n",
    "\n",
    "plt.xlabel('Account level')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Level Account by Gender')\n",
    "plt.legend(title='Gender', loc='best');\n",
    "plt.savefig('Level_Account_by_Gender.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graphic shows that there are more users with a paid account. \n",
    "And most users are women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What about the location of the users? Does this feature affects on the level of service?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location_level = df_valid.select('userId','state','level').groupby('state','level').count().orderBy(desc('count')).toPandas()\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.ylabel('Number of Users')\n",
    "ax = fig.gca()\n",
    "df_location_level.pivot(index='state', columns='level', values='count').plot(kind='bar', ax=ax)\n",
    "plt.title('Users and Account types by State')\n",
    "plt.savefig('df_state_level.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous graphic shows there are more users (also with paid accounts) in California (CA), New York (NY) and Texas (TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What about the UserAgent? Is there a difference between subscriptors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's see the different browsers the users login, that indicates different Operating Systems from users\n",
    "df_os = df_valid.select('userAgent','userId').distinct().groupby('userAgent').count().toPandas().sort_values(by='count')\n",
    "df_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_userAgent_level = df_valid.select('userId','userAgent','level').groupby('userAgent','level').count().toPandas().sort_values(by='count')\n",
    "df_userAgent_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe it's interesting augment our dataset with the device the users utilize our Sparkify service\n",
    "# let's map the operating system, through the Browser\n",
    "\n",
    "map = {'macintosh':'MAC', 'iphone':'IPHONE','ipad':'IPAD',\n",
    "       'x11':'LINUX','compatible':'WINDOWS',\n",
    "       'windows nt 5.0':'WINDOWS','windows nt 5.1':'WINDOWS',\n",
    "       'windows nt 6.1':'WINDOWS', 'windows nt 6.0':'WINDOWS',\n",
    "       'windows nt 6.2':'WINDOWS','windows nt 6.3':'WINDOWS'}\n",
    "\n",
    "classify_os = udf(lambda x: map[re.findall('\\(([^\\)]*)\\)', x)[0].split(';')[0].lower()])\n",
    "\n",
    "df_valid = df_valid.withColumn('OS', classify_os(df_valid.userAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we would like to know the count of OS's connections depending on the subscription level of the users\n",
    "df_os_level = df_valid.select('userId','OS','level').groupby('OS','level').count().orderBy(desc('count')).toPandas()\n",
    "df_os_level.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But for unique userid's, what is the favourite OS?\n",
    "df_os_level = df_valid.select('userId','OS','level').distinct().groupby('OS','level').count().orderBy(desc('count')).toPandas()\n",
    "df_os_level.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous table, we can see that Windows is the favourite Operating Systems for both free and paid subscription\n",
    "#### And on the below table we can confirm by number of distinct users (remember there are 225 users during the time range of this analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_test = df_valid.select('userId','OS').distinct().groupby('OS').count().orderBy(desc('count')).toPandas()\n",
    "df_os_test.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level subscription of the user by Operating System\n",
    "level_os = df_valid.dropDuplicates(['userId','OS','level']).groupby(['OS', 'level']).count().orderBy(desc('count')).toPandas()\n",
    "\n",
    "ax = sns.barplot(x='level', y='count',hue='OS', data=level_os);\n",
    "plt.xlabel('Subscription type')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('What OS do Clients use by subscription type?')\n",
    "plt.legend(title='OS', loc='best');\n",
    "plt.savefig('df_OS_by_subscription_type.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above table shows graphically the OS the users connect to our Sparkify service, both for free and paid subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see the number of songs a users listens to during the day (24 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour = df_valid.filter(df_valid.page == \"NextSong\").groupby(df_valid.hour).count().orderBy(df_valid.hour.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "songs_in_hour_pd.hour = pd.to_numeric(songs_in_hour_pd.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(songs_in_hour_pd[\"hour\"], songs_in_hour_pd[\"count\"])\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Songs played\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"pastel\")\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='hour', y='count', data=songs_in_hour_pd, color=\"b\")\n",
    "plt.title('Number of Songs listened by hour')\n",
    "plt.ylabel('count');\n",
    "plt.savefig('df_number_songs_by_hour.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see if there are days when people listen more songs\n",
    "songs_in_days = df_valid.filter(df_valid.page == \"NextSong\").groupby(df_valid.day_of_month).count().orderBy(df_valid.day_of_month.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate how many songs people listen per month\n",
    "songs_in_days_pd = songs_in_days.toPandas()\n",
    "songs_in_days_pd.day_of_month = pd.to_numeric(songs_in_days_pd.day_of_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_days_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And show these numbers with a nice plot\n",
    "sns.set_color_codes(\"pastel\")\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='day_of_month', y='count', data=songs_in_days_pd, color=\"g\")\n",
    "plt.title('Number of Songs listened by day of month')\n",
    "plt.ylabel('count');\n",
    "plt.savefig('df_number_songs_by_day_of_month.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question: Is there a day of week when users listen more songs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the query\n",
    "songs_in_weekdays = df_valid.filter(df_valid.page == \"NextSong\").groupby(df_valid.day_of_week).count().orderBy(df_valid.day_of_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and convert it to pandas to display \n",
    "songs_in_weekdays_pd = songs_in_weekdays.toPandas()\n",
    "songs_in_weekdays.day_of_week = pd.to_numeric(songs_in_weekdays_pd.day_of_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_weekdays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot with a barplot\n",
    "sns.set_color_codes(\"pastel\")\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='day_of_week', y='count', data=songs_in_weekdays_pd, color=\"y\")\n",
    "plt.title('Number of Songs listened by day of week')\n",
    "plt.ylabel('count');\n",
    "plt.savefig('df_number_songs_by_day_of_week.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It seems that users prefer weekdays to use the service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### With the graphic above, we cannot see a behavioural pattern \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users Downgrade Their Accounts\n",
    "\n",
    "To find when users downgrade their accounts, let's use a window function and cumulative sum to distinguish each user's data as either pre or post downgrade events and then flag those log entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's calculate the users who downgrade the service, that is, the users that downgraded or cancellate the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downgrade_select = udf(lambda x: 1 if x == 'Submit Downgrade' else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid.withColumn('downgrade_select', downgrade_select('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With the previous commands, we get only the time when the user downgraded, \n",
    "###### but we would like to have the information of that user from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have all information from users that have downgraded Sparkify service\n",
    "# so we need to save an interval of time\n",
    "windowSpec  = Window.partitionBy('userId')\n",
    "\n",
    "df_valid = df_valid.withColumn('will_downgrade', max('downgrade_select').over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downgrade = df_valid.select('userId','gender','downgrade_select').distinct().groupby('gender','downgrade_select').count().toPandas().sort_values(by='count')\n",
    "df_downgrade.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Women are slightly more likely to downgrade than men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downgrade_level = df_valid.select('userId','level','downgrade_select').distinct().groupby('level','downgrade_select').count().toPandas().sort_values(by='count')\n",
    "df_downgrade_level.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's calculate the users who churn, that is, the users that confirmed cancellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_confirmation = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df_valid = df_valid.withColumn(\"churn\", churn_confirmation(df_valid.page))\n",
    "df_users_churned = df_valid.withColumn(\"churned\", churn_confirmation(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid.withColumn('will_churn', max('churn').over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remember the schema of the data\n",
    "df_valid.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How many users churned? And what's the rate of churned users vs the total number of users ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count the number of user cancellations (churns) --> there are 52 users who have cancel the service\n",
    "users_churned = df_users_churned.filter(df_users_churned[\"churned\"] ==1).count()\n",
    "# Let's calculate the total number of users\n",
    "total_users = df_valid.select('userId').distinct().count()\n",
    "# Let's calculate the percentage\n",
    "percentage_churn = df_users_churned.groupby(\"userId\").agg({\"churned\":\"sum\"}).select(avg(\"sum(churned)\")).collect()[0][\"avg(sum(churned))\"]\n",
    "\n",
    "print(\"Number of users who churned: {} vs Total Users: {}\".format(users_churned, total_users))\n",
    "print(\"Percentage of users who churned: {:.2f}%\".format(percentage_churn * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With those numbers, we can affirm that we have an imbalanced dataset...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some details of the users who churned\n",
    "df_users_churned.select([\"userId\", \"gender\", \"level\", \"state\", \"ts\"]).where(df_valid.churn == 1).sort(\"ts\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the UserId duplicates and then count the users who churned\n",
    "# And prepare the data to be plotted with a sns, so we need to convert to a Pandas Dataframe\n",
    "total_churn = df_valid.dropDuplicates(['userId','churn']).groupby(['churn']).count().toPandas()\n",
    "print(\"Number of users who churned: {} \".format(total_churn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We like to see it graphically\n",
    "sns.barplot(x='churn', y='count', data=total_churn);\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('The number of Users who churned or not')\n",
    "plt.legend(title='Churn', loc='best');\n",
    "plt.savefig('Users_who_churned.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is there a difference in the gender between the users who churned and who did not churn? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the churn flag and also the gender\n",
    "df_churn_gender = df_valid.select('userId','gender','churn').distinct().groupby('gender','churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare a query, convert the sorted data to pandas \n",
    "df_churn_gender = df_valid.select('userId','gender','churn').distinct().groupby('gender','churn').count().toPandas().sort_values(by='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='gender', y='count',hue='churn', data=df_churn_gender);\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Gender by Churn')\n",
    "plt.legend(title='Churn', loc='best');\n",
    "plt.savefig('Gender by Churn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is there a difference in the subscription type between the users who churned and who did not churn? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_level = df_valid.select('userId','level','churn').distinct().groupby('level','churn').count().toPandas().sort_values(by='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='level', y='count',hue='churn', data=df_churn_level);\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Level by Churn')\n",
    "plt.legend(title='Churn', loc='best');\n",
    "plt.savefig('Level by Churn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is there a difference in the location between the users who churned and who did not churn? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_location = df_valid.select('userId','state','churn').distinct().groupby('state','churn').count().orderBy(desc('count')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "g = sns.barplot(x='state', y='count',hue='churn', data=df_churn_location);\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Location by Churn')\n",
    "plt.legend(title='Churn', loc='best');\n",
    "plt.savefig('Location by Churn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is there a difference in the day_of_week between the users who churned and who did not churn? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week_by_churn = df_valid.dropDuplicates(['userId','day_of_week']).groupby(['day_of_week','churn']).count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "g = sns.barplot(x=\"day_of_week\",y=\"count\", hue=\"churn\", data=day_of_week_by_churn, ax=ax);\n",
    "plt.xlabel('Day_of_week')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Day_of_Week by Churn')\n",
    "plt.legend(title='Churn', loc='best');\n",
    "plt.savefig('Day_of_week by Churn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The data is very imbalanced, but it seems that the favourite day to churn is Monday (day_of_week = 0)\n",
    "##### Maybe the responsibles of Sparkify could prepare some campaigns for Mondays (free trials, discounts, offers, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select(['userId','page','churn','downgrade_select']).groupby(['page','downgrade_select']).count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_total_downgraded = df_valid.dropDuplicates(['userId','page']).groupby(['page','downgrade_select']).count().toPandas()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "g = sns.barplot(x='page', y='count',hue='downgrade_select', data=page_total_downgraded);\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Page')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Activities of the User')\n",
    "plt.legend(title='Downgraded', loc='best');\n",
    "plt.savefig('Activities_User_downgraded.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Question: Is there a difference in the actions between the users who churned and those who did not churn? \n",
    "There are less movements on the users that are going to churn, less Thumbs-Up, less Thumbs-Down, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_total_churn = df_valid.dropDuplicates(['userId','page']).groupby(['page','will_churn']).count().toPandas()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "g = sns.barplot(x='page', y='count',hue='will_churn', data=page_total_churn);\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Page')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Activities of the User')\n",
    "plt.legend(title='Churn', loc='best');\n",
    "plt.savefig('Activities_User_Page.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try sparksql \n",
    "# To do that, we need to create a temporary table, where we'll perform the SQL queries\n",
    "df_valid.createOrReplaceTempView(\"Sparkify_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What are the most played songs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's use spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_artists = spark.sql('''\n",
    "        SELECT song, count(song) AS TOP_SONGS\n",
    "        FROM Sparkify_sql\n",
    "        GROUP BY song\n",
    "        ORDER BY TOP_SONGS DESC\n",
    "        LIMIT 10\n",
    "         \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What are the artist, users listen the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_artists = spark.sql('''\n",
    "        SELECT artist, count(artist) AS TOP_ARTIST\n",
    "        FROM Sparkify_sql\n",
    "        GROUP BY artist\n",
    "        ORDER BY TOP_ARTIST DESC\n",
    "        LIMIT 10\n",
    "         \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What is the average length of song, users listen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_length = spark.sql('''\n",
    "        SELECT length\n",
    "        FROM Sparkify_sql\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(songs_length.toPandas().dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The average duration is about 250 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionId_total_churn = df_valid.dropDuplicates(['userId','will_churn','sessionId']).groupby(['userId','will_churn']).count().toPandas()\n",
    "\n",
    "# compare two groups of users\n",
    "g = sns.FacetGrid(sessionId_total_churn, col=\"will_churn\", sharey=False)\n",
    "g.map(plt.hist, \"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The users who don't churn use the Sparkify service much more than users who will churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionId_total_churn.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a record with a user with the time log, page and level\n",
    "df_valid.select([\"userId\", \"firstname\", \"ts\", \"page\", \"level\"]).where(df_valid.userId == \"125\").sort(\"ts\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see more information about the 52 users who churned\n",
    "df_valid.select([\"userId\", \"firstname\", \"ts\", \"page\", \"level\"]).where(df_valid.churn == 1).sort(\"ts\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the max \n",
    "avg_time_user = df_valid.agg({\"itemInSession\": \"max\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is there a difference in time between the users who churned and who did not churn? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_membership = df_valid.select([\"userId\", \"registration\", \"ts\", \"churn\"]) \\\n",
    "    .withColumn('membership_time',(df_valid.ts-df_valid.registration)) \\\n",
    "    .groupBy('userId', 'churn') \\\n",
    "    .agg({'membership_time':'max'}) \\\n",
    "    .withColumnRenamed('max(membership_time)', 'membership_time') \\\n",
    "    .select('userId', (col('membership_time')/1000/86400).alias('membership_time'),'churn') \\\n",
    "    .sort('membership_time') \\\n",
    "    .toPandas()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_membership, y='membership_time', x='churn', orient='v')\n",
    "plt.ylabel('Membership time in days until churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.title('Membership time (churn/no churn)')\n",
    "sns.despine(ax=ax);\n",
    "plt.savefig('Membership_time.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With the previous boxplot graphic we can see that users who churn\n",
    "##### stay less time in the Sparkify service than those who don't churn\n",
    "##### Users who churn stay 50 days in average\n",
    "##### Users who don't churn stay aprox. 75 days in average "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's remember the fields of our datset\n",
    "# There are some fields that we can transform from categorical to numerical: e.g: gender, level\n",
    "df_valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These are the features I will include to build the model\n",
    "Features:\n",
    " |-- userId: string (nullable = true)\n",
    " |-- churn: cancellation of the service\n",
    " |-- level: type of subscription + I will convert into number 0/1\n",
    " |-- n_artists: number of artist a user listens to + I will convert into number\n",
    " |-- n_songs:number of songs a user listens to + I will convert into number  \n",
    " |-- n_songs_play:number of songs added to the playlist + I will convert into number   \n",
    " |-- n_thumbs_up: number of thumbs up + I will convert into number\n",
    " |-- n_thumbs_down:  number of thumbs down + I will convert into number \n",
    " |-- n_Errors: long (nullable = true)\n",
    " |-- n_friends: number of friends + I will convert into number \n",
    " |-- n_Rolls: number of spots displayed to the user\n",
    " |-- n_Help: number of visits to help page\n",
    " |-- total_sdays: time of the subscription (in days)\n",
    " |-- n_sessions: number of sesions of a user\n",
    " |-- IPAD: the user connects with an IPAD to our Sparkify service\n",
    " |-- IPHONE: the user connects with an IPHONE to our Sparkify service\n",
    " |-- LINUX: the user connects with a Linux device to our Sparkify service\n",
    " |-- MAC: the user connects with a MAC device to our Sparkify service\n",
    " |-- WINDOWS: the user connects with a WINDOWS to our Sparkify service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features related to the session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. number of sessions by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of the sessions of a user\n",
    "n_sessions = df_valid.select('userId','sessionId').dropDuplicates().groupby('userId').count().withColumnRenamed('count','n_sessions')\n",
    "df_n_sessions = n_sessions.withColumn('n_sessions', n_sessions.n_sessions.cast('bigint'))\n",
    "df_n_sessions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_sessions.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Operating System (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the OS the user connects to the Sparkify session and make one hot encoding with the pivot option\n",
    "df_os = df_valid.select('userId','OS').dropDuplicates().groupby('userId').pivot('OS').agg(count('OS')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features related to the subscription\n",
    "#### 1. subscription days of a user\n",
    "#### 2. subscription cancelled (churn)\n",
    "#### 3. level of subscription (payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the subscription days a user stays in the service\n",
    "subscription_days = df_valid.select('userId','subscription_days').groupBy('userId').agg({'subscription_days':'max'}).withColumnRenamed('max(subscription_days)','total_sdays')\n",
    "df_subscription_days = subscription_days.withColumn('total_sdays', subscription_days.total_sdays.cast('double'))\n",
    "df_subscription_days.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subscription_days.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate if a user churned the service\n",
    "# We need to be careful with this searchs as we can forget basic things:\n",
    "# We have to search through \"churn\" flag, but we will take the max value (1) \n",
    "# because the user before churning, he was subscribed in the service \n",
    "# and so the \"churn\" flag was equal to 0\n",
    "# If we do a fast search with churn=1, the user is repeated\n",
    "churn = df_valid.groupby('userId').max(\"churn\").withColumnRenamed(\"max(churn)\", \"churn\")\n",
    "df_churn = churn.withColumn('churn', churn.churn.cast('int'))\n",
    "df_churn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn.groupby(\"churn\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select the last level a user selected and change level into a numerical,binary field (0/1)\n",
    "level = df_valid.select(['userId', 'level','ts']).orderBy(desc('ts')).dropDuplicates(['userId']).select(['userId', 'level']).replace(['free', 'paid'], ['0', '1'], 'level')\n",
    "\n",
    "## Let's change level into a numerical,binary field (0/1)\n",
    "#level = df_valid.select(['userId', 'level']).dropDuplicates(['userId']).replace(['free', 'paid'], ['0', '1'], 'level')\n",
    "df_level = level.withColumn('level', level.level.cast('int'))\n",
    "df_level.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_level.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features related to the user\n",
    "#### 1. Gender of a user --> I will not include this feature in the model\n",
    "#### 2. Number of artists listened by a user\n",
    "#### 3. Number of songs listened by a user\n",
    "#### 4. Number of songs added to the playlist\n",
    "#### 5. Duration of songs added to the playlist\n",
    "#### 6. Average number of songs a user listens to music during a session\n",
    "#### 7. Number of thumbs_up a user does\n",
    "#### 8. Number of thumbs_down a user does\n",
    "#### 9. Number of friends a user adds\n",
    "#### 10.Subscription days the user has been with our service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change gender into a numerical,binary field (0/1)\n",
    "# But I will not include this feature in the training model\n",
    "gender = df_valid.select(['userId', 'gender']).dropDuplicates(['userId']).replace(['F', 'M'], ['0', '1'], 'gender')\n",
    "df_gender = gender.withColumn('gender', gender.gender.cast('int'))\n",
    "df_gender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of artists a user listens to \n",
    "n_artists = df_valid.filter(df_valid.page==\"NextSong\").select(['userId', 'artist']).dropDuplicates().groupby('userId').count().withColumnRenamed('count', 'n_artists')\n",
    "df_n_artists= n_artists.withColumn('n_artists', n_artists.n_artists.cast('int'))\n",
    "df_n_artists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_artists.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of songs a user listens to \n",
    "n_songs = df_valid.select('userId','song').groupBy('userId').count().withColumnRenamed('count', 'n_songs')\n",
    "df_n_songs= n_songs.withColumn('n_songs', n_songs.n_songs.cast('double'))\n",
    "df_n_songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_songs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of songs a user adds to a playlist\n",
    "n_songs_playlist = df_valid.select('userId','page').where(df_valid.page == 'Add to Playlist').groupBy('userId').count().withColumnRenamed('count', 'n_songs_playlist')\n",
    "df_n_songs_playlist= n_songs_playlist.withColumn('n_songs_playlist', n_songs_playlist.n_songs_playlist.cast('double'))\n",
    "df_n_songs_playlist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_songs_playlist.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the duration of songs a user adds to a playlist\n",
    "n_time_songs = df_valid.select('userId','length').groupBy('userId').sum().withColumnRenamed('sum(length)', 'n_time_songs')\n",
    "df_n_time_songs= n_time_songs.withColumn('n_time_songs', n_time_songs.n_time_songs.cast('double'))\n",
    "df_n_time_songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_time_songs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of songs a user adds to a playlist\n",
    "n_songs_play = df_valid.select('userId', 'page').where(df_valid.page =='Add to Playlist').groupBy('userId').agg({'page':'count'}).withColumnRenamed('count(page)','n_songs_play')\n",
    "df_n_songs_play = n_songs_play.withColumn('n_songs_play', n_songs_play.n_songs_play.cast('double'))\n",
    "df_n_songs_play.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the average number of songs a user listens to music during a session\n",
    "n_avg_songs = df_valid.where('page == \"NextSong\"').groupby(['userId', 'sessionId']).count().groupby(['userId']).agg({'count':'avg'}).withColumnRenamed('avg(count)','n_avg_songs')\n",
    "df_n_avg_songs= n_avg_songs.withColumn('n_avg_songs', n_avg_songs.n_avg_songs.cast('double'))\n",
    "df_n_avg_songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_avg_songs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of thumbs_up a user does\n",
    "n_thumbs_up = df_valid.select('userId','page').where(df_valid.page == 'Thumbs Up').groupBy('userId').count().withColumnRenamed('count', 'n_thumbs_up')\n",
    "df_n_thumbs_up= n_thumbs_up.withColumn('n_thumbs_up', n_thumbs_up.n_thumbs_up.cast('bigint'))\n",
    "df_n_thumbs_up.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_thumbs_up.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of thumbs_down a user does\n",
    "n_thumbs_down = df_valid.select('userId','page').where(df_valid.page == 'Thumbs Down').groupBy('userId').count().withColumnRenamed('count', 'n_thumbs_down')\n",
    "df_n_thumbs_down= n_thumbs_down.withColumn('n_thumbs_down', n_thumbs_down.n_thumbs_down.cast('bigint'))\n",
    "df_n_thumbs_down.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_thumbs_down.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of Error Pages a user experiments\n",
    "n_Errors = df_valid.select('userId','page').where(df_valid.page == 'Error').groupBy('userId').count().withColumnRenamed('count', 'n_Errors')\n",
    "df_n_Errors = n_Errors.withColumn('n_Errors', n_Errors.n_Errors.cast('bigint'))\n",
    "df_n_Errors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_Errors.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of friends a user adds\n",
    "n_friends = df_valid.select('userId','page').where(df_valid.page == 'Add Friend').groupBy('userId').count().withColumnRenamed('count', 'n_friends')\n",
    "df_n_friends= n_friends.withColumn('n_friends', n_friends.n_friends.cast('bigint'))\n",
    "df_n_friends.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_friends.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of Roll Advert a user experiments\n",
    "n_Rolls = df_valid.select('userId','page').where(df_valid.page == 'Roll Advert').groupBy('userId').count().withColumnRenamed('count', 'n_Rolls')\n",
    "df_n_Rolls = n_Rolls.withColumn('n_Rolls', n_Rolls.n_Rolls.cast('bigint'))\n",
    "df_n_Rolls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_Rolls.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the number of Help a user visits\n",
    "n_Help = df_valid.select('userId','page').where(df_valid.page == 'Help').groupBy('userId').count().withColumnRenamed('count', 'n_Help')\n",
    "df_n_Help = n_Help.withColumn('n_Help', n_Help.n_Help.cast('bigint'))\n",
    "df_n_Help.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_Help.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the membership time of a user\n",
    "#df_lifetime = df_valid.select('userId', 'subscription_days')\n",
    "df_subscription_days.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subscription_days.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_sessions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_sessions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's collect all the previous features in a set\n",
    "features = [df_n_sessions, df_total_time, df_subscription_days, df_downgraded,\n",
    "            df_gender, df_level, df_n_artists, df_n_songs, df_n_songs_play,\n",
    "            df_n_time_songs, df_n_avg_songs, df_n_thumbs_up, \n",
    "            df_n_thumbs_down, df_n_Errors, df_n_friends, df_n_Rolls, df_n_Help]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the number of churn users\n",
    "df_valid.groupby(\"churn\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's collect all the previous features in a set\n",
    "\n",
    "final_features = df_churn.join(df_level,'userId','outer')\\\n",
    "     .join(df_n_artists,'userId','outer') \\\n",
    "     .join(df_n_songs,'userId','outer') \\\n",
    "     .join(df_n_songs_play,'userId','outer')\\\n",
    "     .join(df_n_thumbs_up,'userId','outer') \\\n",
    "     .join(df_n_thumbs_down,'userId','outer') \\\n",
    "     .join(df_n_Errors,'userId','outer') \\\n",
    "     .join(df_n_friends,'userId','outer') \\\n",
    "     .join(df_n_Rolls,'userId','outer') \\\n",
    "     .join(df_n_Help,'userId','outer') \\\n",
    "     .join(df_subscription_days,'userId','outer') \\\n",
    "     .join(df_n_sessions,'userId','outer') \\\n",
    "     .join(df_os,'userId','outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a general overview of the final features dataframe\n",
    "final_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if churn proportion maintains (as I had some problems with this)\n",
    "final_features.groupby(\"churn\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the size of the final features dataframe\n",
    "rows = final_features.count()\n",
    "cols = len(final_features.columns)\n",
    "print(\"The number of rows is {} and the number of columns is {}\".format(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need userId, as we will predict if a user will churn, but we are not interested in the id of that user\n",
    "final_features = final_features.drop('userId', 'sessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we need to imputate nulls with a value, 0 for instance\n",
    "final_features= final_features.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of the churn column into label, that will be the feature to predict\n",
    "final_features = final_features.withColumnRenamed(\"churn\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_pandas = numerical_features.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_features = numerical_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's calculate the correlation matrix to see if there are \n",
    "# correlation between features\n",
    "corr = final_features.toPandas().corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a heatmap with the correlation of the variables\n",
    "# we can see there are some correlated features, such as number of songs and the number of artist, \n",
    "# that are related, the number of songs added to the playlist\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,  cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we have to vectorize our features with a VectorAssembler\n",
    "# because SparkML requires a vector of features\n",
    "assembler = VectorAssembler(inputCols=columns_features[1:], outputCol=\"NumFeatures\")\n",
    "df = assembler.transform(numerical_features)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second step is to standardize the features, to avoid that a feature that has higher values,\n",
    "# dominates the rest of features\n",
    "# To do that we use the StandardScaler method (scaling the standard deviation) \n",
    "scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"features\", withStd=True)\n",
    "scalerModel = scaler.fit(df)\n",
    "df = scalerModel.transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We eliminate NumFeatures column as we don't need it anymore\n",
    "df = df.drop('NumFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test 70% vs 30%\n",
    "train, test = df_final.randomSplit([0.7, 0.3], seed = 42)\n",
    "#train, test, validation = df_final.randomSplit([0.6, 0.2, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if we have both label (0/1)\n",
    "train.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if we have both label (0/1)\n",
    "test.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a function to calculate the performance of our models\n",
    "# inspired by https://stackoverflow.com/questions/60772315/how-to-evaluate-a-classifier-with-apache-spark-2-4-5-and-pyspark-python\n",
    "def calculate_performace(results):\n",
    "    '''\n",
    "    Function that calculate the performance of a model\n",
    "    Input:  \n",
    "            model: model we want to evaluate\n",
    "    Output: \n",
    "            print performance metrics\n",
    "    '''  \n",
    "    # Create both evaluators\n",
    "    evaluatorMulti = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction')\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'label', rawPredictionCol='prediction', metricName='areaUnderROC')\n",
    "\n",
    "    # Make predictions\n",
    "    predictionAndTarget = results.select('label', 'prediction')\n",
    "    #print (\"The Ground Truth is {} and the prediction is \".format(target, prediction))    \n",
    "    \n",
    "    # Get metrics\n",
    "    acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "    f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "    auc = evaluator.evaluate(predictionAndTarget)  \n",
    "   \n",
    "    # Prepare a DataFrame with all metrics\n",
    "    performance = pd.DataFrame(index=['Accuracy', 'F1', 'AUC'], \\\n",
    "                     data={'Performance value': [acc, f1, auc]})\n",
    "    \n",
    "    # Return metrics\n",
    "    return performance       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR = LogisticRegression(maxIter=10, regParam=0.0)\n",
    "clf_DT = DecisionTreeClassifier(seed=5)\n",
    "clf_GBT = GBTClassifier(maxDepth=5, maxIter = 10, seed=42)\n",
    "clf_RF = RandomForestClassifier(seed=5)\n",
    "clf_SVM = LinearSVC(maxIter = 10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "############## LOGISTIC REGRESSION ##############\n",
    "# Train the model\n",
    "LR_model = clf_LR.fit(train)\n",
    "# Test the model\n",
    "prediction_LR = LR_model.transform(test)\n",
    "\n",
    "# Let's prepare the confusion matrix\n",
    "# and cast the results to Pandas \n",
    "# in order to use sklearn metrics (confussion matrix and classification report)\n",
    "y_true = prediction_LR.select(\"label\").toPandas()\n",
    "y_pred = prediction_LR.select(\"prediction\").toPandas()\n",
    "\n",
    "conf_matrix_LR = confusion_matrix(y_true, y_pred)\n",
    "clas_rep_LR = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Logistic Regression:\\n\")\n",
    "print(conf_matrix_LR)\n",
    "print(\"\\n\")\n",
    "print(clas_rep_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "############## DECISION TREES ##############\n",
    "# Train the model\n",
    "DT_model = clf_DT.fit(train)\n",
    "# Test the model\n",
    "prediction_DT = DT_model.transform(test)\n",
    "\n",
    "# Let's prepare the confusion matrix\n",
    "# and cast the results to Pandas \n",
    "# in order to use sklearn metrics (confussion matrix and classification report)\n",
    "y_true = prediction_DT.select(\"label\").toPandas()\n",
    "y_pred = prediction_DT.select(\"prediction\").toPandas()\n",
    "\n",
    "conf_matrix_DT = confusion_matrix(y_true, y_pred)\n",
    "clas_rep_DT = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Decission Trees:\\n\")\n",
    "print(conf_matrix_DT)\n",
    "print(\"\\n\")\n",
    "print(clas_rep_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "############## GRADIENT BOOSTED TREE ##############\n",
    "# Train the model\n",
    "GBT_model = clf_GBT.fit(train)\n",
    "# Test the model\n",
    "prediction_GBT = GBT_model.transform(test)\n",
    "\n",
    "# Let's prepare the confusion matrix\n",
    "# and cast the results to Pandas \n",
    "# in order to use sklearn metrics (confussion matrix and classification report)\n",
    "y_true = prediction_GBT.select(\"label\").toPandas()\n",
    "y_pred = prediction_GBT.select(\"prediction\").toPandas()\n",
    "\n",
    "conf_matrix_GBT = confusion_matrix(y_true, y_pred)\n",
    "clas_rep_GBT = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Gradient Boosted Tree:\\n\")\n",
    "print(conf_matrix_GBT)\n",
    "print(\"\\n\")\n",
    "print(clas_rep_GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "############## RANDOM FOREST ##############\n",
    "# Train the model\n",
    "RF_model = clf_RF.fit(train)\n",
    "# Test the model\n",
    "prediction_RF = RF_model.transform(test)\n",
    "\n",
    "# Let's prepare the confusion matrix\n",
    "# and cast the results to Pandas \n",
    "# in order to use sklearn metrics (confussion matrix and classification report)\n",
    "y_true = prediction_RF.select(\"label\").toPandas()\n",
    "y_pred = prediction_RF.select(\"prediction\").toPandas()\n",
    "\n",
    "conf_matrix_RF = confusion_matrix(y_true, y_pred)\n",
    "clas_rep_RF = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Random Forest:\\n\")\n",
    "print(conf_matrix_RF)\n",
    "print(\"\\n\")\n",
    "print(clas_rep_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "############## SUPPORT VECTOR MACHINE ##############\n",
    "# Train the model\n",
    "SVM_model = clf_SVM.fit(train)\n",
    "# Test the model\n",
    "prediction_SVM = SVM_model.transform(test)\n",
    "\n",
    "# Let's prepare the confusion matrix\n",
    "# and cast the results to Pandas \n",
    "# in order to use sklearn metrics (confussion matrix and classification report)\n",
    "y_true = prediction_SVM.select(\"label\").toPandas()\n",
    "y_pred = prediction_SVM.select(\"prediction\").toPandas()\n",
    "\n",
    "conf_matrix_SVM = confusion_matrix(y_true, y_pred)\n",
    "clas_rep_SVM = classification_report(y_true, y_pred)\n",
    "\n",
    "print(\"Support Vector Machine:\\n\")\n",
    "print(conf_matrix_SVM)\n",
    "print(\"\\n\")\n",
    "print(clas_rep_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Trees model has the best F1-score\n",
    "# Let's see the importance feature of this model\n",
    "# to do that, first we get the indices of the feature importances and convert them to a list\n",
    "feat_imp_ind   = GBT_model.featureImportances.indices.tolist()\n",
    "# and we get the feature names, from the original final_features except the label (churn)\n",
    "features_without_label = final_features.columns[1:]\n",
    "feat_imp_key   = [features_without_label[ind] for ind in feat_imp_ind]\n",
    "# after that we get the weight, that is, the importance value and convert them to a list\n",
    "feat_imp_value = GBT_model.featureImportances.values.tolist()\n",
    "# next we get all together in a dataframe, to be able to display it with a nice sns barplot\n",
    "feat_df        = pd.DataFrame(list(zip(features_without_label,feat_imp_value)),columns=['Feature','Importance']).sort_values('Importance',ascending=False)\n",
    "# prepare the graphical details\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Feature Importance on Gradient Boosted Trees model\")\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_df);\n",
    "plt.savefig('Feature_Importance_DT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now I am going to fine-tune the Decision Tree model\n",
    "# as it is the one with the highest F1-Score\n",
    "# Inspiration in https://gist.github.com/colbyford/7758088502211daa90dbc1b51c408762\n",
    "\n",
    "# Create the initial GBT Model\n",
    "\n",
    "dt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=2)\n",
    "\n",
    "# Prepare the parameters to find the best combination\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxBins,[2, 4]) \\\n",
    "    .addGrid(dt.maxDepth,[2, 4]) \\\n",
    "    .build()\n",
    "\n",
    "# Evaluate the model\n",
    "dtevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "dtcv = CrossValidator(estimator = dt,\n",
    "                      estimatorParamMaps = dt_param_grid,\n",
    "                      evaluator = dtevaluator,\n",
    "                      numFolds = 3)\n",
    "\n",
    "# Run Cross Validations\n",
    "dtcvModel = dtcv.fit(train)\n",
    "print(dtcvModel)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "dtpredictions = dtcvModel.transform(test)\n",
    "\n",
    "# Evaluate best model\n",
    "print('Accuracy:', dtevaluator.evaluate(dtpredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = dtcvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_pred = best_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_best = best_model_pred.select(\"label\").toPandas()\n",
    "y_pred_best = best_model_pred.select(\"prediction\").toPandas()\n",
    "\n",
    "conf_matrix_GBT_best = confusion_matrix(y_true_best, y_pred_best)\n",
    "clas_rep_GBT_best = classification_report(y_true_best, y_pred_best)\n",
    "\n",
    "print(\"Best Gradient Boosted Trees:\\n\")\n",
    "print(conf_matrix_DT_best)\n",
    "print(\"\\n\")\n",
    "print(clas_rep_DT_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model after the hyperparameter tuning is not better than the original\n",
    "# So we will keep the original as the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to execute it on the larger dataset, I will prepare some functions\n",
    "# based on the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_spark(path_to_dataset):\n",
    "    '''\n",
    "    Function that load a dataset\n",
    "    Input:  \n",
    "            path_to_dataset: path to the file containing the data\n",
    "    Output: \n",
    "            df: loaded dataset\n",
    "    '''  \n",
    "    df = spark.read.json(path_to_dataset)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineerme(df_input):\n",
    "    '''\n",
    "    Function that collects the feature to build the ML model\n",
    "    Input:  \n",
    "            df_input: dataframe\n",
    "    Output: \n",
    "            df_output: output datadrame with the features\n",
    "    '''  \n",
    "    # This dataframe will contain the following features:\n",
    "    # |-- userId: string (nullable = true)\n",
    "    # |-- churn: cancellation of the service\n",
    "    # |-- level: type of subscription + I will convert into number 0/1\n",
    "    # |-- n_artists: number of artist a user listens to + I will convert into number\n",
    "    # |-- n_songs:number of songs a user listens to + I will convert into number  \n",
    "    # |-- n_songs_play:number of songs added to the playlist + I will convert into number   \n",
    "    # |-- n_thumbs_up: number of thumbs up + I will convert into number\n",
    "    # |-- n_thumbs_down:  number of thumbs down + I will convert into number \n",
    "    # |-- n_Errors: long (nullable = true)\n",
    "    # |-- n_friends: number of friends + I will convert into number \n",
    "    # |-- n_Rolls: number of spots displayed to the user\n",
    "    # |-- n_Help: number of visits to help page\n",
    "    # |-- total_sdays: time of the subscription (in days)\n",
    "    # |-- n_sessions: number of sesions of a user\n",
    "    # |-- IPAD: the user connects with an IPAD to our Sparkify service\n",
    "    # |-- IPHONE: the user connects with an IPHONE to our Sparkify service\n",
    "    # |-- LINUX: the user connects with a Linux device to our Sparkify service\n",
    "    # |-- MAC: the user connects with a MAC device to our Sparkify service\n",
    "    # |-- WINDOWS: the user connects with a WINDOWS to our Sparkify service\n",
    "\n",
    "\n",
    "    churn = df_input.groupby('userId').max(\"churn\").withColumnRenamed(\"max(churn)\", \"churn\")\n",
    "    df_churn = churn.withColumn('churn', churn.churn.cast('int'))\n",
    "    df_churn.count()\n",
    "        \n",
    "    level = df_input.select(['userId', 'level','ts']).orderBy(desc('ts')).dropDuplicates(['userId']).select(['userId', 'level']).replace(['free', 'paid'], ['0', '1'], 'level')\n",
    "    ## Let's change level into a numerical,binary field (0/1)\n",
    "    #level = df_valid.select(['userId', 'level']).dropDuplicates(['userId']).replace(['free', 'paid'], ['0', '1'], 'level')\n",
    "    df_level = level.withColumn('level', level.level.cast('int'))\n",
    "    df_level.count()\n",
    "    \n",
    "    # Let's calculate the number of artists a user listens to \n",
    "    n_artists = df_input.filter(df_valid.page==\"NextSong\").select(['userId', 'artist']).dropDuplicates().groupby('userId').count().withColumnRenamed('count', 'n_artists')\n",
    "    df_n_artists= n_artists.withColumn('n_artists', n_artists.n_artists.cast('int'))\n",
    "    df_n_artists.count()\n",
    "    \n",
    "    n_songs = df_input.select('userId','song').groupBy('userId').count().withColumnRenamed('count', 'n_songs')\n",
    "    df_n_songs= n_songs.withColumn('n_songs', n_songs.n_songs.cast('double'))\n",
    "    df_n_songs.count()\n",
    "    \n",
    "    n_songs_play = df_input.select('userId', 'page').where(df_valid.page =='Add to Playlist').groupBy('userId').agg({'page':'count'}).withColumnRenamed('count(page)','n_songs_play')\n",
    "    df_n_songs_play = n_songs_play.withColumn('n_songs_play', n_songs_play.n_songs_play.cast('double'))\n",
    "    df_n_songs_play.count()\n",
    "        \n",
    "    n_thumbs_up = df_input.select('userId','page').where(df_valid.page == 'Thumbs Up').groupBy('userId').count().withColumnRenamed('count', 'n_thumbs_up')\n",
    "    df_n_thumbs_up= n_thumbs_up.withColumn('n_thumbs_up', n_thumbs_up.n_thumbs_up.cast('bigint'))\n",
    "    df_n_thumbs_up.count()\n",
    "    \n",
    "    n_thumbs_down = df_input.select('userId','page').where(df_valid.page == 'Thumbs Down').groupBy('userId').count().withColumnRenamed('count', 'n_thumbs_down')\n",
    "    df_n_thumbs_down= n_thumbs_down.withColumn('n_thumbs_down', n_thumbs_down.n_thumbs_down.cast('bigint'))\n",
    "    df_n_thumbs_down.count()\n",
    "    \n",
    "    n_Errors = df_input.select('userId','page').where(df_valid.page == 'Error').groupBy('userId').count().withColumnRenamed('count', 'n_Errors')\n",
    "    df_n_Errors = n_Errors.withColumn('n_Errors', n_Errors.n_Errors.cast('bigint'))\n",
    "    df_n_Errors.count()\n",
    "    \n",
    "    n_friends = df_input.select('userId','page').where(df_valid.page == 'Add Friend').groupBy('userId').count().withColumnRenamed('count', 'n_friends')\n",
    "    df_n_friends= n_friends.withColumn('n_friends', n_friends.n_friends.cast('bigint'))\n",
    "    df_n_friends.count()\n",
    "    \n",
    "    n_Rolls = df_input.select('userId','page').where(df_valid.page == 'Roll Advert').groupBy('userId').count().withColumnRenamed('count', 'n_Rolls')\n",
    "    df_n_Rolls = n_Rolls.withColumn('n_Rolls', n_Rolls.n_Rolls.cast('bigint'))\n",
    "    df_n_Rolls.count()\n",
    "    \n",
    "    # Let's calculate the number of Help a user visits\n",
    "    n_Help = df_input.select('userId','page').where(df_valid.page == 'Help').groupBy('userId').count().withColumnRenamed('count', 'n_Help')\n",
    "    df_n_Help = n_Help.withColumn('n_Help', n_Help.n_Help.cast('bigint'))\n",
    "    df_n_Help.count()\n",
    "    \n",
    "    # Let's calculate the subscription days a user stays in the service\n",
    "    subscription_days = df_input.select('userId','subscription_days').groupBy('userId').agg({'subscription_days':'max'}).withColumnRenamed('max(subscription_days)','total_sdays')\n",
    "    df_subscription_days = subscription_days.withColumn('total_sdays', subscription_days.total_sdays.cast('double'))\n",
    "    df_subscription_days.count()\n",
    "\n",
    "    n_sessions = df_input.select('userId','sessionId').dropDuplicates().groupby('userId').count().withColumnRenamed('count','n_sessions')\n",
    "    df_n_sessions = n_sessions.withColumn('n_sessions', n_sessions.n_sessions.cast('bigint'))\n",
    "    df_n_sessions.count()\n",
    "    \n",
    "    # Let's take the OS the user connects to the Sparkify session and make one hot encoding with the pivot option\n",
    "    df_os = df_valid.select('userId','OS').dropDuplicates().groupby('userId').pivot('OS').agg(count('OS')).fillna(0)   \n",
    "    \n",
    "    df_output = df_churn.join(df_level,'userId','outer')\\\n",
    "     .join(df_n_artists,'userId','outer') \\\n",
    "     .join(df_n_songs,'userId','outer') \\\n",
    "     .join(df_n_songs_play,'userId','outer')\\\n",
    "     .join(df_n_thumbs_up,'userId','outer') \\\n",
    "     .join(df_n_thumbs_down,'userId','outer') \\\n",
    "     .join(df_n_Errors,'userId','outer') \\\n",
    "     .join(df_n_friends,'userId','outer') \\\n",
    "     .join(df_n_Rolls,'userId','outer') \\\n",
    "     .join(df_n_Help,'userId','outer') \\\n",
    "     .join(df_subscription_days,'userId','outer') \\\n",
    "     .join(df_n_sessions,'userId','outer') \\\n",
    "     .join(df_os,'userId','outer')\n",
    "    \n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_feat = feature_engineerme(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
